{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "377341d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "import torch\n",
    "from typing import List, Dict, Any\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from embedder import Embedder\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb102650",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = transforms.ToTensor()\n",
    "resize = transforms.Resize(\n",
    "    (224, 224),\n",
    "    transforms.InterpolationMode.BILINEAR,\n",
    "    antialias=True,\n",
    ")\n",
    "\n",
    "def embed_crops(\n",
    "    embedder, crops: List[torch.Tensor]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Embeds a list of crops using the embedder model.\n",
    "    :param crops: List of crops as torch tensors.\n",
    "    :param resize_size: Size to resize the crops to before embedding.\n",
    "    :return: Embedded vectors as a torch tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    return embedder.embed(crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "092b2ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32673419",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_root = Path(\"./model\")\n",
    "embedder = Embedder(trunk_weights=model_root / \"trunk_weights.pth\",\n",
    "            embedder_weights=model_root / \"embedder_weights.pth\",\n",
    "            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd819a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:08<00:41,  8.39s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(image_paths), batch_size)):\n\u001b[32m     10\u001b[39m     batch_paths = image_paths[i:i + batch_size]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     batch_images = [resize(\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m).to(device) \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m batch_paths]\n\u001b[32m     12\u001b[39m     batch_tensors = torch.stack(batch_images)\n\u001b[32m     13\u001b[39m     batch_vectors = embed_crops(embedder, batch_tensors)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pipeline/lib/python3.12/site-packages/torchvision/transforms/transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pipeline/lib/python3.12/site-packages/torchvision/transforms/functional.py:168\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[32m    167\u001b[39m mode_to_nptype = {\u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m\"\u001b[39m: np.int32, \u001b[33m\"\u001b[39m\u001b[33mI;16\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys.byteorder == \u001b[33m\"\u001b[39m\u001b[33mlittle\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mI;16B\u001b[39m\u001b[33m\"\u001b[39m: np.int16, \u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m: np.float32}\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m img = torch.from_numpy(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pic.mode == \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    171\u001b[39m     img = \u001b[32m255\u001b[39m * img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pipeline/lib/python3.12/site-packages/PIL/Image.py:747\u001b[39m, in \u001b[36mImage.__array_interface__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    745\u001b[39m     new[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.tobytes(\u001b[33m\"\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m747\u001b[39m     new[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    748\u001b[39m new[\u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m], new[\u001b[33m\"\u001b[39m\u001b[33mtypestr\u001b[39m\u001b[33m\"\u001b[39m] = _conv_type_shape(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pipeline/lib/python3.12/site-packages/PIL/Image.py:796\u001b[39m, in \u001b[36mImage.tobytes\u001b[39m\u001b[34m(self, encoder_name, *args)\u001b[39m\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_name == \u001b[33m\"\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m encoder_args == ():\n\u001b[32m    794\u001b[39m     encoder_args = \u001b[38;5;28mself\u001b[39m.mode\n\u001b[32m--> \u001b[39m\u001b[32m796\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.width == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.height == \u001b[32m0\u001b[39m:\n\u001b[32m    799\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pipeline/lib/python3.12/site-packages/PIL/ImageFile.py:250\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n\u001b[32m    248\u001b[39m             \u001b[38;5;28mself\u001b[39m.map = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m err_code = -\u001b[32m3\u001b[39m  \u001b[38;5;66;03m# initialize to unknown error\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.map:\n\u001b[32m    253\u001b[39m     \u001b[38;5;66;03m# sort tiles in file order\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pipeline/lib/python3.12/site-packages/PIL/ImageFile.py:326\u001b[39m, in \u001b[36mImageFile.load_prepare\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    324\u001b[39m     \u001b[38;5;66;03m# create image memory if necessary\u001b[39;00m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m         \u001b[38;5;28mself\u001b[39m.im = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnew\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    327\u001b[39m     \u001b[38;5;66;03m# create palette (optional)\u001b[39;00m\n\u001b[32m    328\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "det_path = Path(\"/Users/iman/345-data/ml-datasets/ccbf/recognition/ccbf-20250324-231335-det renders v2-angles\")\n",
    "\n",
    "images_dir = Path(\"/Users/iman/345-data/ml-datasets/ccbf/recognition/ccbf-20250324-231335-det renders v2-fixed\")\n",
    "for angle in range(0,15):\n",
    "    vectors_output_path = det_path / f\"det_vectors_{angle}\"\n",
    "    batch_size = 1000\n",
    "    all_vectors = []\n",
    "    image_paths = sorted(list(Path(f\"/Users/iman/345-data/ml-datasets/ccbf/recognition/ccbf-20250324-231335-det renders v2-angles/det renders {angle}\").glob(\"*.jpg\")))\n",
    "    for i in tqdm(range(0, len(image_paths), batch_size)):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        batch_images = [resize(to_tensor(Image.open(path))).to(device) for path in batch_paths]\n",
    "        batch_tensors = torch.stack(batch_images)\n",
    "        batch_vectors = embed_crops(embedder, batch_tensors)\n",
    "        all_vectors.append(batch_vectors.cpu())  # Move to CPU immediately\n",
    "        del batch_tensors, batch_vectors\n",
    "    # Concatenate all batch results\n",
    "    vectors = torch.cat(all_vectors, dim=0)\n",
    "    torch.save(vectors, vectors_output_path.with_suffix(\".pt\"))\n",
    "\n",
    "classes_output_path = det_path / \"det_classes_angles.json\"\n",
    "image_paths = sorted(list(Path(f\"/Users/iman/Downloads/det renders {angle}\").glob(\"*.jpg\")))\n",
    "\n",
    "# Extract class names from image paths\n",
    "class_names = [\"_\".join(path.stem.split(\"_\")[:-1]) for path in image_paths]\n",
    "\n",
    "# Save as JSON\n",
    "with open(classes_output_path, 'w') as f:\n",
    "    json.dump(class_names, f, indent=2)\n",
    "\n",
    "print(f\"Saved {len(class_names)} class names to {classes_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vectors = []\n",
    "for angle in range(0,15):\n",
    "    vectors = torch.load(Path(det_path / f\"det_vectors_{angle}\").with_suffix(\".pt\"), map_location=\"cpu\")\n",
    "    all_vectors.append(vectors)\n",
    "all_vectors = torch.stack(all_vectors, dim=0)\n",
    "\n",
    "with open(det_path / \"det_classes_angles.json\") as f:\n",
    "    all_classes = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f582b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5984, 256])\n",
      "tensor([[-0.0024, -0.0078,  0.0130,  ...,  0.0026, -0.0177, -0.0138],\n",
      "        [-0.0131, -0.0113, -0.0019,  ...,  0.0155, -0.0185, -0.0164],\n",
      "        [-0.0123, -0.0091, -0.0005,  ...,  0.0170, -0.0158, -0.0155],\n",
      "        [-0.0084, -0.0185,  0.0038,  ..., -0.0117, -0.0135, -0.0122],\n",
      "        [-0.0116, -0.0230, -0.0155,  ...,  0.0102,  0.0236,  0.0156]])\n",
      "5984\n",
      "['002cd8f0-7908-44ac-a688-d17d626cbb16_00078000003659_back', '002cd8f0-7908-44ac-a688-d17d626cbb16_00078000003659_front', '002cd8f0-7908-44ac-a688-d17d626cbb16_00078000003659_left', '002cd8f0-7908-44ac-a688-d17d626cbb16_00078000003659_right', '00467eba-8f78-4e8a-9ee9-7f8232bf361e_00810014530345_back']\n",
      "torch.Size([2566, 256])\n",
      "tensor([[-0.0389,  0.0474, -0.0156,  ...,  0.0650, -0.0370,  0.0159],\n",
      "        [-0.0479, -0.0581, -0.0751,  ..., -0.0274, -0.0260,  0.0655],\n",
      "        [-0.0666, -0.0245, -0.0247,  ..., -0.0045,  0.0244,  0.0335],\n",
      "        [-0.0579, -0.0683, -0.1388,  ..., -0.0103, -0.0062,  0.0094],\n",
      "        [ 0.0555, -0.0218,  0.0484,  ...,  0.0218,  0.0554,  0.0910]])\n",
      "2566\n",
      "['007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_front', '007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_left', '007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_right', '00bd46a6-fb25-4e3d-8053-a1b4dc6b5508_00052000051179_front', '00d1a842-30c8-45a8-9f94-fd424e673a16_00070847012474_front']\n"
     ]
    }
   ],
   "source": [
    "VECTORS_DIR = Path(\"/Users/iman/345-data/ml-datasets/ccbf/vectors\")\n",
    "TEST_DATASET = \"ccbf-20241127-20250326\"\n",
    "DATASET = \"ccbf-20250324-231335-det renders v2-angles\"\n",
    "det_svs_path = VECTORS_DIR / DATASET \n",
    "\n",
    "det_vectors = torch.load(det_svs_path / \"supervectors_det.pt\", map_location=\"cpu\")\n",
    "print(det_vectors.shape)\n",
    "print(det_vectors[:5])\n",
    "\n",
    "with open(det_svs_path / \"classes.json\") as f:\n",
    "    det_classes = json.load(f)\n",
    "\n",
    "print(len(det_classes))\n",
    "print(det_classes[:5])\n",
    "\n",
    "test_vectors_dir = os.path.join(VECTORS_DIR, TEST_DATASET, \"test set\")\n",
    "\n",
    "test_vectors = torch.load(\n",
    "    os.path.join(test_vectors_dir, \"testvectors.pt\"), map_location=\"cpu\"\n",
    ")\n",
    "print(test_vectors.shape)\n",
    "print(test_vectors[:5])\n",
    "\n",
    "with open(os.path.join(test_vectors_dir, \"classes.json\")) as f:\n",
    "    test_classes = json.load(f)\n",
    "\n",
    "print(len(test_classes))\n",
    "print(test_classes[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082edaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorise somewhere\n",
    "# mla_vectors = [out.vector for out in valid_outputs]\n",
    "# mla_gt_classes = [out.gt_label for out in valid_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6802d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_upcs = [cls.split(\"_\")[1] for cls in det_classes]\n",
    "\n",
    "cls_to_vec = {\n",
    "    cls: vec\n",
    "    for cls, vec in zip(mla_gt_classes, mla_vectors)\n",
    "    if cls in det_upcs\n",
    "}\n",
    "mla_filtered_classes = [\n",
    "    cls for cls in mla_gt_classes if cls in det_upcs and cls != \"missing\" and cls != \"\"\n",
    "]\n",
    "mla_filtered_vectors = torch.stack(\n",
    "    [cls_to_vec[cls] for cls in mla_gt_classes if cls != \"missing\" and cls != \"\"]\n",
    ")\n",
    "print(len(mla_filtered_vectors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465b88df",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "distances = []\n",
    "for angle in range(0,15):\n",
    "\n",
    "    mean_dist = torch.mean(torch.diag(torch.cdist(det_vectors, all_vectors[angle], p=2)))\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "    knn.fit(all_vectors[angle], all_classes)\n",
    "\n",
    "    predicted_labels = knn.predict(mla_filtered_vectors.cpu().numpy())\n",
    "    correct_predictions = sum(\n",
    "        [\n",
    "            1\n",
    "            for gt, pred in zip(mla_filtered_classes, predicted_labels)\n",
    "            if gt == pred.split(\"_\")[1]\n",
    "        ]\n",
    "    )\n",
    "    accuracy = correct_predictions / len(mla_filtered_classes)\n",
    "    accuracies.append(accuracy)\n",
    "    distances.append(mean_dist)\n",
    "\n",
    "    print(f\"Mean distance, image {angle}: {mean_dist:.4f}. Accuracy: {100*accuracy:.2f}%\")\n",
    "for i, angle in enumerate(range(0, 15)):\n",
    "    plt.annotate(f'{angle}', (distances[i], accuracies[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "plt.scatter(distances, accuracies, marker='o')\n",
    "plt.xlabel('Mean Distance')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlim(0.815,0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba5606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
