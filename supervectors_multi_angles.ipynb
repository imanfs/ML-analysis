{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a8f2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from shutil import copy\n",
    "from pathlib import Path\n",
    "from shutil import copy\n",
    "import asyncio\n",
    "import io\n",
    "import aiohttp\n",
    "from typing import List\n",
    "from PIL import Image, ImageOps\n",
    "from utils import download_images\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e081ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "import torch\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from embedder import Embedder\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "629d666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/Users/iman/345-data/\"\n",
    "DATA_DIR = Path(ROOT_DIR, \"ml-datasets\", \"ccbf\")\n",
    "VECTORS_DIR = Path(DATA_DIR, \"vectors\")\n",
    "\n",
    "DATASET = \"ccbf-det-fronts-20250709\"\n",
    "output_dir = VECTORS_DIR / DATASET\n",
    "crops_dir = DATA_DIR / \"recognition\" / DATASET\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "crops_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5841f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da926337",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_root = Path(\"./model\")\n",
    "\n",
    "\n",
    "embedder = Embedder(model_root / \"trunk_weights.pth\",\n",
    "                    model_root / \"embedder_weights.pth\",device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e4fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "resize = transforms.Resize((224, 224),interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "images = [Image.open(img) for img in sorted((crops_dir).rglob(\"*.jpg\"))]\n",
    "images_tensors = torch.stack([resize(to_tensor(img)) for img in images]).to(device)\n",
    "\n",
    "vectors = embedder.embed(images_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a9084bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:17<00:00,  9.71s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size  = 1500          # ← tweak to taste\n",
    "from tqdm import tqdm\n",
    "to_tensor   = transforms.ToTensor()\n",
    "resize      = transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "\n",
    "# Load paths only (delay decoding until we’re inside the batch loop)\n",
    "image_paths = sorted(Path(crops_dir).rglob(\"*.jpg\"))\n",
    "\n",
    "all_vecs = []                         # will hold the per-batch outputs\n",
    "classes = []                     # will hold the per-batch classes\n",
    "for start in tqdm(range(0, len(image_paths), batch_size)):\n",
    "    batch_paths = image_paths[start:start + batch_size]\n",
    "\n",
    "    batch_imgs = [\n",
    "        resize(to_tensor(Image.open(p).convert(\"RGB\")))  # ensure 3-channel\n",
    "        for p in batch_paths\n",
    "    ]\n",
    "    batch_tensor = torch.stack(batch_imgs).to(device)\n",
    "\n",
    "    batch_vecs = embedder.embed(batch_tensor)            # (B, …)\n",
    "    all_vecs.append(batch_vecs.cpu())                    # keep on CPU to save GPU mem\n",
    "    batch_classes = [path.stem for path in batch_paths]\n",
    "    classes.extend(batch_classes)\n",
    "# (N, …) stacked result\n",
    "vectors = torch.cat(all_vecs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af1f4dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vectors, output_dir / \"vectors.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba93b849",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = torch.load(output_dir / \"vectors.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f2e27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = [img.stem for img in sorted((crops_dir).rglob(\"*.jpg\"))]\n",
    "json.dump(classes, open(output_dir / \"classes.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "499df2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_vectors = vectors\n",
    "ref_classes = image_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df4d95d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3656/3656 [00:00<00:00, 47406.01it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ref_classes\n",
    "## need to normalize the supervectors\n",
    "unique_classes, class_indices = np.unique([\"_\".join(cls.split(\"_\")[:-1]) for cls in ref_classes], return_inverse=True)\n",
    "unique_uuids, uuid_indices = np.unique(\n",
    "    [\"_\".join(cls.split(\"_\")[:-2]) for cls in ref_classes], return_inverse=True\n",
    ")\n",
    "num_of_classes = class_indices[-1] + 1\n",
    "num_uuids = uuid_indices[-1] + 1\n",
    "#supervectors = torch.empty((num_of_classes, 256), dtype=ref_vectors.dtype)\n",
    "supervectors = torch.empty((num_of_classes, 256))\n",
    "for class_i in tqdm(range(num_of_classes)):\n",
    "    vectors = ref_vectors[class_indices == class_i]\n",
    "    supervectors[class_i, :] = torch.mean(vectors, axis=0)\n",
    "\n",
    "supervectors /= torch.linalg.norm(supervectors, axis=1)[:, torch.newaxis]\n",
    "superclasses = unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d468db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    supervectors,\n",
    "    output_dir / \"supervectors.pt\",\n",
    ")\n",
    "with open(\n",
    "    output_dir / \"superclasses.json\", \"w\"\n",
    ") as f:\n",
    "    json.dump(superclasses.tolist(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6b584cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sample_split(vectors, classes, face=None):\n",
    "    print(vectors.shape, classes.shape)\n",
    "    if isinstance(vectors,torch.Tensor):\n",
    "        vectors = np.array(vectors.cpu().numpy())\n",
    "        classes = np.array(classes)\n",
    "    # if face is not None, create a mask for the face, applied to all\n",
    "    if face is not None:\n",
    "        vectors, classes = filter_by_face(vectors, classes, face)\n",
    "    print(vectors.shape, classes.shape)\n",
    "    # ## offset elements by 1 so that the last element of each face is not included in training set\n",
    "    mask = np.concatenate(([False], classes[:-1] != classes[1:]))\n",
    "    # split the vectors and classes into train and sample sets\n",
    "    print(vectors.shape, classes.shape, mask.shape)\n",
    "    train_vectors = vectors[~mask, :]\n",
    "    train_classes = classes[~mask]\n",
    "\n",
    "    sample_vectors = vectors[mask, :]\n",
    "    sample_classes = classes[mask]\n",
    "\n",
    "    return train_vectors, train_classes, sample_vectors, sample_classes\n",
    "\n",
    "def create_face_mask(classes, face):\n",
    "    if isinstance(face, (tuple, list)):\n",
    "        return np.array([any(f in label for f in face) for label in classes])\n",
    "    else:\n",
    "        return np.array([face in label for label in classes])\n",
    "\n",
    "def filter_by_face(vectors, classes, face):\n",
    "    face_mask = np.array([cls.split(\"_\")[-1] == face for cls in classes])\n",
    "    return vectors[face_mask], np.array(classes)[face_mask]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "56a95ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_front', 'bf906b27-9d58-4a9b-8602-e395ebf49bef_00021136180596_front'), ('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_left', 'bf906b27-9d58-4a9b-8602-e395ebf49bef_00021136180596_front'), ('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_right', 'bf906b27-9d58-4a9b-8602-e395ebf49bef_00021136180596_front')]\n",
      "1308\n",
      "Accuracy for face='front': 77.60%\n",
      "[('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_front', 'bf906b27-9d58-4a9b-8602-e395ebf49bef_00021136180596_left'), ('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_left', 'bf906b27-9d58-4a9b-8602-e395ebf49bef_00021136180596_left'), ('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_right', 'bf906b27-9d58-4a9b-8602-e395ebf49bef_00021136180596_left')]\n",
      "1254\n",
      "Accuracy for face='left': 61.24%\n",
      "[('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_front', '91c5e7e6-7b6b-4a54-b797-3f47bd512519_00074806001615_right'), ('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_left', 'bf906b27-9d58-4a9b-8602-e395ebf49bef_00021136180596_right'), ('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_right', 'c47b6e4f-7704-456c-89e9-f0be3f5e8293_00858176002423_right')]\n",
      "1258\n",
      "Accuracy for face='right': 58.11%\n",
      "[('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_front', 'eea13c8b-ec92-495e-9925-6b8e341f5518_00858176002171_back'), ('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_left', 'bf906b27-9d58-4a9b-8602-e395ebf49bef_00021136180596_back'), ('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_right', 'bf906b27-9d58-4a9b-8602-e395ebf49bef_00021136180596_back')]\n",
      "1228\n",
      "Accuracy for face='back': 53.34%\n",
      "[('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_front', 'bf906b27-9d58-4a9b-8602-e395ebf49bef_00021136180596_front'), ('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_left', 'bf906b27-9d58-4a9b-8602-e395ebf49bef_00021136180596_right'), ('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_right', 'bf906b27-9d58-4a9b-8602-e395ebf49bef_00021136180596_back')]\n",
      "1318\n",
      "Accuracy for face=None: 79.51%\n"
     ]
    }
   ],
   "source": [
    "## train knn for specific faces\n",
    "sample_vectors = torch.load('/Users/iman/345-data/ml-datasets/ccbf/vectors/ccbf-20241127-20250326/test set/testvectors.pt')\n",
    "sample_classes = json.load(open('/Users/iman/345-data/ml-datasets/ccbf/vectors/ccbf-20241127-20250326/test set/classes.json', 'r'))\n",
    "\n",
    "sample_vectors = sample_vectors.cpu().numpy()\n",
    "sample_classes = np.array(sample_classes)\n",
    "\n",
    "ref_vectors = vectors\n",
    "ref_classes = image_names\n",
    "\n",
    "for face in (\"front\", \"left\", \"right\", \"back\", None):\n",
    "    if face:\n",
    "        train_vectors, train_classes = filter_by_face(ref_vectors.cpu().numpy(), ref_classes, face)\n",
    "    else:\n",
    "        train_vectors = ref_vectors.cpu().numpy()\n",
    "        train_classes = np.array(ref_classes)\n",
    "    # fit knn to ref set\n",
    "    knn = KNeighborsClassifier(n_neighbors=1, metric=\"euclidean\")\n",
    "    knn.fit(train_vectors, train_classes)\n",
    "\n",
    "    predicted_labels = knn.predict(sample_vectors)\n",
    "    print([(gt, pred) for gt, pred in zip(sample_classes[:3], predicted_labels[:3])])\n",
    "    train_upcs = [cls.split(\"_\")[1] for cls in train_classes]\n",
    "    correct_predictions = sum(\n",
    "        [\n",
    "            1\n",
    "            for gt, pred in zip(sample_classes, predicted_labels)\n",
    "            if gt.split(\"_\")[1] == pred.split(\"_\")[1] if gt.split(\"_\")[1] in train_upcs\n",
    "        ]\n",
    "    )\n",
    "    total_test_classes = [cls.split(\"_\")[1] for cls in sample_classes if cls.split(\"_\")[1] in train_upcs]\n",
    "    print(len(total_test_classes))\n",
    "    accuracy = correct_predictions / len(total_test_classes)\n",
    "\n",
    "    print(f\"Accuracy for {face=}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# which ones fail?\n",
    "len(sample_classes) - correct_predictions\n",
    "\n",
    "incorrect_predictions = [\n",
    "    (gt, pred)\n",
    "    for gt, pred in zip(sample_classes, predicted_labels)\n",
    "    if gt.split(\"_\")[1] != pred.split(\"_\")[1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a6ccf035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_front', 'c2c64acb-2391-4620-8e98-6b895f1318f8_00021136010374_front'), ('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_left', '007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_front'), ('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_right', '007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_front')]\n",
      "2565\n",
      "Accuracy for face='front': 87.21%\n",
      "[('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_front', 'c2c64acb-2391-4620-8e98-6b895f1318f8_00021136010374_left'), ('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_left', '007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_left'), ('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_right', 'c2c64acb-2391-4620-8e98-6b895f1318f8_00021136010374_left')]\n",
      "2565\n",
      "Accuracy for face='left': 76.65%\n",
      "[('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_front', 'c2c64acb-2391-4620-8e98-6b895f1318f8_00021136010374_right'), ('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_left', '007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_right'), ('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_right', '64b23ed9-848c-4561-9fe5-e1bc0bc4c545_00021136016444_right')]\n",
      "2565\n",
      "Accuracy for face='right': 74.66%\n",
      "[('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_front', 'c2c64acb-2391-4620-8e98-6b895f1318f8_00021136010374_back'), ('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_left', '007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_back'), ('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_right', '007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_back')]\n",
      "2565\n",
      "Accuracy for face='back': 52.48%\n",
      "[('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_front', 'c2c64acb-2391-4620-8e98-6b895f1318f8_00021136010374_front'), ('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_left', '007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_left'), ('007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_right', '007a6be0-4ed0-4950-88e7-dd0547f50127_00021136180596_back')]\n",
      "2565\n",
      "Accuracy for face=None: 89.51%\n"
     ]
    }
   ],
   "source": [
    "## train knn for specific faces\n",
    "train_vectors = torch.load('/Users/iman/345-data/ml-datasets/ccbf/vectors/ccbf-20241127-20250326/reference/refvectors.pt')\n",
    "train_classes = json.load(open('/Users/iman/345-data/ml-datasets/ccbf/vectors/ccbf-20241127-20250326/reference/classes.json', 'r'))\n",
    "\n",
    "ref_vectors = train_vectors\n",
    "ref_classes = train_classes\n",
    "\n",
    "for face in (\"front\", \"left\", \"right\", \"back\", None):\n",
    "    if face:\n",
    "        train_vectors, train_classes = filter_by_face(ref_vectors.cpu().numpy(), ref_classes, face)\n",
    "    else:\n",
    "        train_vectors = ref_vectors.cpu().numpy()\n",
    "        train_classes = np.array(ref_classes)\n",
    "    # fit knn to ref set\n",
    "    knn = KNeighborsClassifier(n_neighbors=1, metric=\"euclidean\")\n",
    "    knn.fit(train_vectors, train_classes)\n",
    "\n",
    "    predicted_labels = knn.predict(sample_vectors)\n",
    "    print([(gt, pred) for gt, pred in zip(sample_classes[:3], predicted_labels[:3])])\n",
    "    train_upcs = [cls.split(\"_\")[1] for cls in train_classes]\n",
    "    correct_predictions = sum(\n",
    "        [\n",
    "            1\n",
    "            for gt, pred in zip(sample_classes, predicted_labels)\n",
    "            if gt.split(\"_\")[1] == pred.split(\"_\")[1] if gt.split(\"_\")[1] in train_upcs\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    total_test_classes = [cls.split(\"_\")[1] for cls in sample_classes if cls.split(\"_\")[1] in train_upcs]\n",
    "    print(len(total_test_classes))\n",
    "    accuracy = correct_predictions / len(total_test_classes)\n",
    "\n",
    "    print(f\"Accuracy for {face=}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# which ones fail?\n",
    "len(sample_classes) - correct_predictions\n",
    "\n",
    "incorrect_predictions = [\n",
    "    (gt, pred)\n",
    "    for gt, pred in zip(sample_classes, predicted_labels)\n",
    "    if gt.split(\"_\")[1] != pred.split(\"_\")[1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "035b3434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('Users/iman')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\"Users/iman\") / \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "97d56a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "if path:\n",
    "    print(\"yes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
